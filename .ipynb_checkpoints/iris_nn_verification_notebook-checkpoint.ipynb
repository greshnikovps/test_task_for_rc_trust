{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81ee760",
   "metadata": {},
   "source": [
    "### Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ea7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch scikit-learn onnx onnx2torch onnxruntime z3-solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d443a1",
   "metadata": {},
   "source": [
    "### Fitting a neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6ba3e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Loss: 1.1007\n",
      "Epoch [40/200], Loss: 1.0687\n",
      "Epoch [60/200], Loss: 1.0240\n",
      "Epoch [80/200], Loss: 0.9570\n",
      "Epoch [100/200], Loss: 0.8711\n",
      "Epoch [120/200], Loss: 0.7801\n",
      "Epoch [140/200], Loss: 0.6954\n",
      "Epoch [160/200], Loss: 0.6190\n",
      "Epoch [180/200], Loss: 0.5498\n",
      "Epoch [200/200], Loss: 0.4851\n",
      "Test Accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import onnx\n",
    "\n",
    "# fixing random seed\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()  \n",
    "X, y = iris.data, iris.target  \n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()  \n",
    "X = scaler.fit_transform(X)  \n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define the neural network model\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the neural network.\n",
    "        \"\"\"\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 10)  # First fully connected layer (input: 4, output: 10)\n",
    "        self.fc2 = nn.Linear(10, 10)  # Second fully connected layer (input: 10, output: 10)\n",
    "        self.fc3 = nn.Linear(10, 3)  # Third fully connected layer (input: 10, output: 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "        \n",
    "        Arguments:\n",
    "        x -- input tensor\n",
    "        \n",
    "        Returns:\n",
    "        Tensor containing the network's output after passing through the layers and activation functions.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the output of the first layer\n",
    "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the output of the second layer\n",
    "        x = self.fc3(x)  # Pass through the third layer (no activation)\n",
    "        return x  # Return the final output\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = IrisNet()\n",
    "criterion = nn.CrossEntropyLoss()  # Use cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Use Adam optimizer with a learning rate of 0.001\n",
    "\n",
    "# Train the network\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    \n",
    "# Export the model to ONNX format\n",
    "dummy_input = torch.randn(1, 4)  # Create a dummy input with the same shape as the network's input\n",
    "\n",
    "# Export the trained model to the ONNX format for use in other frameworks or tools\n",
    "torch.onnx.export(model, dummy_input, \"iris_model.onnx\", input_names=['input'], output_names=['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd7b1b",
   "metadata": {},
   "source": [
    "### Implementation of a neural network verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf545d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from onnx2torch import convert\n",
    "from z3 import *\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"iris_model.onnx\")  # Load the ONNX model from file\n",
    "model = convert(onnx_model)  # Convert the ONNX model to a PyTorch model\n",
    "\n",
    "\n",
    "def get_weights_and_biases(model):\n",
    "    \"\"\"\n",
    "    Extracts the weights and biases from the converted PyTorch model.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The converted PyTorch model.\n",
    "    \n",
    "    Returns:\n",
    "    A tuple containing the weights and biases for each layer of the model.\n",
    "    \"\"\"\n",
    "    # Access the layers and extract weights and biases\n",
    "    weights_1 = model._modules['fc1/Gemm'].weight.detach().numpy()\n",
    "    bias_1 = model._modules['fc1/Gemm'].bias.detach().numpy()\n",
    "    weights_2 = model._modules['fc2/Gemm'].weight.detach().numpy()\n",
    "    bias_2 = model._modules['fc2/Gemm'].bias.detach().numpy()\n",
    "    weights_3 = model._modules['fc3/Gemm'].weight.detach().numpy()\n",
    "    bias_3 = model._modules['fc3/Gemm'].bias.detach().numpy()\n",
    "    \n",
    "    return (weights_1, bias_1), (weights_2, bias_2), (weights_3, bias_3)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation function for Z3.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Input value or list of values.\n",
    "    \n",
    "    Returns:\n",
    "    The ReLU-transformed value(s).\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [If(element > 0, element, 0) for element in x]\n",
    "    else:\n",
    "        return If(x > 0, x, 0)\n",
    "\n",
    "    \n",
    "def Linear(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Implements the linear layer computation (Wx + b) for Z3.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Input list of Z3 Real variables.\n",
    "    weights -- Weights matrix for the layer.\n",
    "    bias -- Bias vector for the layer.\n",
    "    \n",
    "    Returns:\n",
    "    The output list after applying the linear transformation.\n",
    "    \"\"\"\n",
    "    output = [sum(x[i] * weights[j][i] for i in range(len(x))) + bias[j] for j in range(len(weights))]\n",
    "    return output\n",
    "\n",
    "\n",
    "def encode_network_in_z3(solver, model, input_var):\n",
    "    \"\"\"\n",
    "    Encodes the neural network layers in Z3 using the provided weights, biases, and input variables.\n",
    "    \n",
    "    Arguments:\n",
    "    solver -- Z3 solver instance.\n",
    "    model -- The converted PyTorch model.\n",
    "    input_var -- List of Z3 Real variables representing the input.\n",
    "    \n",
    "    Returns:\n",
    "    The final output of the network encoded in Z3.\n",
    "    \"\"\"\n",
    "    # Get weights and biases\n",
    "    (weights_1, bias_1), (weights_2, bias_2), (weights_3, bias_3) = get_weights_and_biases(model)\n",
    "    \n",
    "    # Encode each layer using Z3 operations\n",
    "    x = ReLU(Linear(input_var, weights_1, bias_1))\n",
    "    x = ReLU(Linear(x, weights_2, bias_2))\n",
    "    x = Linear(x, weights_3, bias_3)\n",
    "    return x\n",
    "\n",
    "def add_precondition(solver, data_point, epsilon):\n",
    "    \"\"\"\n",
    "    Adds the precondition to the solver ensuring that the input variables are within an epsilon neighborhood of the original data point.\n",
    "    \n",
    "    Arguments:\n",
    "    solver -- Z3 solver instance.\n",
    "    data_point -- The original data point.\n",
    "    epsilon -- The maximum allowed perturbation (epsilon).\n",
    "    \"\"\"\n",
    "    for i in range(len(data_point)):\n",
    "        solver.add(Real(f'x{i}') >= data_point[i] - epsilon)\n",
    "        solver.add(Real(f'x{i}') <= data_point[i] + epsilon)\n",
    "\n",
    "def add_postcondition1(solver, encoded_output, original_class):\n",
    "    \"\"\"\n",
    "    Adds the postcondition to the solver ensuring that the predicted class does not change.\n",
    "    \n",
    "    Arguments:\n",
    "    solver -- Z3 solver instance.\n",
    "    encoded_output -- The output of the network encoded in Z3.\n",
    "    original_class -- The original predicted class.\n",
    "    \"\"\"\n",
    "    for i in range(len(encoded_output)):\n",
    "        if i != original_class:\n",
    "            solver.add(encoded_output[original_class] > encoded_output[i])\n",
    "            \n",
    "def add_postcondition(solver, encoded_output, original_class):\n",
    "    \"\"\"\n",
    "    Adds the negation of the postcondition to the solver ensuring that the predicted class does not change.\n",
    "    \n",
    "    Arguments:\n",
    "    solver -- Z3 solver instance.\n",
    "    encoded_output -- The output of the network encoded in Z3.\n",
    "    original_class -- The original predicted class.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    conditions = []\n",
    "    for i in range(len(encoded_output)):\n",
    "        if i != original_class:\n",
    "            conditions.append(encoded_output[original_class] <= encoded_output[i])\n",
    "    solver.add(Or(conditions))\n",
    "\n",
    "\n",
    "def verify_robustness(model, data_point, epsilon):\n",
    "    \"\"\"\n",
    "    Verifies the robustness of the model for a given data point and epsilon.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The converted PyTorch model.\n",
    "    data_point -- The original data point.\n",
    "    epsilon -- The maximum allowed perturbation (epsilon).\n",
    "    \n",
    "    Returns:\n",
    "    \"Counterexample\" and the counterexample if a counterexample is found (i.e., the network is not robust),\n",
    "    \"Verified\" if the network is robust for the given epsilon.\n",
    "    \"\"\"\n",
    "    solver = Solver()\n",
    "    \n",
    "    # Convert data point to Z3 Real variables\n",
    "    input_vars = [Real(f'x{i}') for i in range(len(data_point))]\n",
    "\n",
    "    # Add the precondition (input bounds)\n",
    "    add_precondition(solver, data_point, epsilon)\n",
    "\n",
    "    # Encode the network in Z3\n",
    "    encoded_output = encode_network_in_z3(solver, model, input_vars)\n",
    "\n",
    "    # Determine the original class\n",
    "    original_class = torch.argmax(model(torch.tensor(data_point, dtype=torch.float32))).item()\n",
    "\n",
    "    # Add the postcondition (output class should not change)\n",
    "    add_postcondition(solver, encoded_output, original_class)\n",
    "\n",
    "    # Check the constraints\n",
    "    if solver.check() == sat:\n",
    "        return \"Counterexample\", solver.model()\n",
    "    else:\n",
    "        return \"Verified\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde18424",
   "metadata": {},
   "source": [
    "### Verify the neural network you trained in Step 1. Use the first data point from the IRIS dataset and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e986396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.1: Verified\n",
      "Epsilon = 2: ('Counterexample', [x0 = 109931882970219/100000000000000,\n",
      " x3 = 10969803076489903753901175769628847792550482099948183460699787989512010379212752942908329093472266218689033727538799501/18348881582738574355362874331627473349473315701219589932628446983620540242060599596417903277845217758026937500000000000,\n",
      " x1 = 33228834348116071468626650002980862312446914528252032944197648074307074186887306359324333681385436776036700155631756363/469731368518107503497289582889663317746516881951221502275288242780685830196751349668298323912837574605489600000000000000,\n",
      " x2 = -3678728751529687465591958156836165239430898479814643773972351714511497427834614330002410102709290283270373342884852691209/1174328421295268758743223957224158294366292204878053755688220606951714575491878374170745809782093936513724000000000000000])\n"
     ]
    }
   ],
   "source": [
    "data_point = X[0]  \n",
    "epsilon_values = [0.1, 2]  \n",
    "for epsilon in epsilon_values:\n",
    "    result = verify_robustness(model, data_point, epsilon)\n",
    "    print(f'Epsilon = {epsilon}: {result}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f868b9",
   "metadata": {},
   "source": [
    "So for Epsilon = 0.1 model is robust, while for Epsilon = 2 counterexample found. Now let's iteratively find maximum (with a certain error associated with discretization) Epsilon for which model is still robust. We will use binary search algorithm for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1693eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.8510 is the maximal (with tolerance=0.0001) epsilon value for which the model is still robust.\n"
     ]
    }
   ],
   "source": [
    "def print_max_robust_eps(model, data_point, start=0.1, end=2.0, tolerance=0.0001):\n",
    "    \"\"\"\n",
    "    Finds and prints the maximum epsilon value for which the model is still robust using binary search.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The converted PyTorch model.\n",
    "    data_point -- The original data point.\n",
    "    start -- The starting epsilon value for the search (default is 0.1).\n",
    "    end -- The ending epsilon value for the search (default is 2.0).\n",
    "    tolerance -- The stopping criterion for the binary search (default is 0.0001).\n",
    "    \n",
    "    Returns:\n",
    "    The maximum epsilon value for which the model is still robust.\n",
    "    \"\"\"\n",
    "    max_verified_epsilon = 0  # Initialize the maximum verified epsilon\n",
    "\n",
    "    while abs(end - start) >= tolerance:\n",
    "        mid = (start + end) / 2  # Calculate the midpoint\n",
    "        result = verify_robustness(model, data_point, mid)\n",
    "\n",
    "        if result == \"Verified\":\n",
    "            max_verified_epsilon = mid\n",
    "            start = mid + tolerance  # Move the start to mid + tolerance to search the higher range\n",
    "        else:\n",
    "            end = mid - tolerance  # Move the end to mid - tolerance to search the lower range\n",
    "\n",
    "    if max_verified_epsilon == 0:\n",
    "        print(\"The model is not verified even for the minimal epsilon.\")\n",
    "    else:\n",
    "        print(f\"Epsilon = {max_verified_epsilon:.4f} is the maximal (with tolerance={tolerance}) epsilon value for which the model is still robust.\")\n",
    "\n",
    "data_point = X[0]\n",
    "print_max_robust_eps(model, data_point, start=0.1, end=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
